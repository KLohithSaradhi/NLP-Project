{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"DanceScripts.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  96089\n",
      "Total Vocab:  33\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  95989\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text[i:i + seq_length]\n",
    " seq_out = raw_text[i + seq_length]\n",
    " dataX.append([char_to_int[char] for char in seq_in])\n",
    " dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-21-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "filename = \"./weights-improvement-08-0.3936.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:43:33.134498: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-29 14:43:33.288829: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/750 [..............................] - ETA: 27s - loss: 0.3483  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:43:33.443033: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - ETA: 0s - loss: 0.3990\n",
      "Epoch 1: loss improved from inf to 0.39899, saving model to weights-improvement-21-01-0.3990.hdf5\n",
      "750/750 [==============================] - 26s 33ms/step - loss: 0.3990\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 2.7473\n",
      "Epoch 2: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 25s 33ms/step - loss: 2.7473\n",
      "Epoch 3/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 3.1083\n",
      "Epoch 3: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 33ms/step - loss: 3.1082\n",
      "Epoch 4/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.9402\n",
      "Epoch 4: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.9400\n",
      "Epoch 5/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.9465\n",
      "Epoch 5: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.9468\n",
      "Epoch 6/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.9027\n",
      "Epoch 6: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.9025\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 2.8227\n",
      "Epoch 7: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 25s 33ms/step - loss: 2.8227\n",
      "Epoch 8/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.7767\n",
      "Epoch 8: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.7767\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 2.7461\n",
      "Epoch 9: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.7461\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 2.6274\n",
      "Epoch 10: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.6274\n",
      "Epoch 11/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 3.0053\n",
      "Epoch 11: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 3.0052\n",
      "Epoch 12/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.8916\n",
      "Epoch 12: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 23s 31ms/step - loss: 2.8915\n",
      "Epoch 13/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.7714\n",
      "Epoch 13: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.7713\n",
      "Epoch 14/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.6307\n",
      "Epoch 14: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.6308\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 2.4365\n",
      "Epoch 15: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.4365\n",
      "Epoch 16/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 2.1540\n",
      "Epoch 16: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 2.1536\n",
      "Epoch 17/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 1.8412\n",
      "Epoch 17: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 31ms/step - loss: 1.8413\n",
      "Epoch 18/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 1.5861\n",
      "Epoch 18: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 1.5857\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 1.3847\n",
      "Epoch 19: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 1.3847\n",
      "Epoch 20/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 1.2372\n",
      "Epoch 20: loss did not improve from 0.39899\n",
      "750/750 [==============================] - 24s 31ms/step - loss: 1.2370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x34c9be4f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 29, 11, 18, 9, 21, 19, 11, 1]\n",
      "\"                                                                                             welcome  \"\n",
      "tturise phetii to the siglesiag \n",
      "pet's seleome soe soage and eancst \n",
      "\n",
      "poepare to be daztled by the radiant charm of luna! with her infectious energy and captivating soage presence, she'll lrane you sheolbo tiee woan bedce  she'll take you on a journey tfrough the masie \n",
      "\n",
      "erace yourselves for the explosive energy of blaze! with his fiery passion and eantivating srage presence, she'll lrane you sheolbo tie  with  with his siart oo ce cadtis ti  hatie st the moage with a performance that iesveset t\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "seed = \"welcome \"\n",
    "seed = \" \" * (seq_length - len(seed)) + seed\n",
    "pattern = [char_to_int[char] for char in seed]\n",
    "print(\"Seed:\", pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(500):\n",
    " x = np.reshape(pattern, (1, len(pattern), 1))\n",
    " x = x / float(n_vocab)\n",
    " prediction = model.predict(x, verbose=0)\n",
    " index = np.argmax(prediction)\n",
    " result = int_to_char[index]\n",
    " seq_in = [int_to_char[value] for value in pattern]\n",
    " sys.stdout.write(result)\n",
    " pattern.append(index)\n",
    " pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
